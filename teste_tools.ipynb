{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45b171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "# Importe suas fun√ß√µes de teste\n",
    "# from test_pipeline import test_full_workflow \n",
    "\n",
    "# --- Utilit√°rio de Formata√ß√£o ---\n",
    "def format_bytes(size):\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return f\"{size:.2f} {power_labels[n]}\"\n",
    "\n",
    "# --- Classe Monitor de Mem√≥ria ---\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.running = False\n",
    "        self.max_rss = 0\n",
    "        self.start_rss = 0\n",
    "        self.end_rss = 0\n",
    "        self._thread = None\n",
    "\n",
    "    def _monitor(self):\n",
    "        while self.running:\n",
    "            # RSS: Resident Set Size (Mem√≥ria RAM f√≠sica usada)\n",
    "            current_rss = self.process.memory_info().rss\n",
    "            if current_rss > self.max_rss:\n",
    "                self.max_rss = current_rss\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        self.start_rss = self.process.memory_info().rss\n",
    "        self.max_rss = self.start_rss\n",
    "        self.running = True\n",
    "        self._thread = threading.Thread(target=self._monitor, daemon=True)\n",
    "        self._thread.start()\n",
    "        print(f\"üìâ Mem√≥ria Inicial: {format_bytes(self.start_rss)}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self._thread:\n",
    "            self._thread.join()\n",
    "        self.end_rss = self.process.memory_info().rss\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"üìä RELAT√ìRIO DE MEM√ìRIA DO PROCESSO\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"üìâ Inicial:      {format_bytes(self.start_rss)}\")\n",
    "        print(f\"üìà Final:        {format_bytes(self.end_rss)}\")\n",
    "        print(f\"üöÄ PICO (Peak):  {format_bytes(self.max_rss)}\")\n",
    "        print(f\"üíß Diferen√ßa:    {format_bytes(self.end_rss - self.start_rss)}\")\n",
    "        print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ef27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 out of 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\.venv\\Lib\\site-packages\\motor\\core.py:171: UserWarning: You appear to be connected to a DocumentDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/documentdb\n",
      "  delegate = self.__delegate_class__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garantindo √≠ndices...\n",
      "üìâ Mem√≥ria Inicial: 517.51 MB\n",
      "‚è≥ Executando Workflow...\n",
      "\n",
      "üîπ --- INICIANDO TESTE DE INTEGRA√á√ÉO (SEM LLM) ---\n",
      "\n",
      "Testing: 1. Orchestrator - Discovery (SQL)\n",
      "‚úÖ SQL Result: ['qta46nlsd6', 'wern8rs7b1']\n",
      "Usando Hash Alvo: uw0qfu4a1r\n",
      "\n",
      "Testing: 2. Orchestrator - Ingestion (Mongo -> Context)\n",
      "DEBUG [Context]: Mongo Data Loaded. Rows: 1000\n",
      "‚úÖ Load Result: SUCCESS: Loaded 1000 requests into AnalysisContext.\n",
      "Sources: google | Hashes: 1\n",
      "Action Required: Delegate to 'Metrics Analyst' agent to run ML inference now.\n",
      "Status:  Mongo Raw: 1000 | ML Processed: Pending\n",
      "\n",
      "Testing: 3. Sub-agent - ML Execution\n",
      "Embedding type:  fasttext\n",
      "[DEBUG] Model Path FASTTEXT: c:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr/files/models/embedding/fasttext_google.model\n",
      "Enter to Fasttext encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Criando Vocabul√°rio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 2418.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 out of 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vetorizando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:07<00:00, 133.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing encoding\n",
      "(1000, 100)\n",
      "DEBUG [Context]: ML Results Stored. Rows: 1000\n",
      "checando se a tool de inferencia salva dos dados: 1000\n",
      "‚úÖ Inference Result: Inference completed using 'google' model with results: \n",
      "Models Accuracy: 0.902Total Error in prediction (possible anomalies): 98Analyzed 1000 samples.\n",
      "You can now now:\n",
      "1. Call 'get_dataset_health_check' to see overall performance stats.\n",
      "2. Call 'query_anomalous_ids' to extract specific samples for the Detective Agent.\n",
      "\n",
      "Testing: 4. Sub-agent - Health Check\n",
      "‚úÖ Health Stats: {'total_samples': 1000, 'false_positives': 51, 'false_negatives': 47, 'avg_trust': 0.894597053527832}\n",
      "\n",
      "Testing: 5. Sub-agent - Query Anomalies\n",
      "‚ùå Query Failed: 'id'\n",
      "\n",
      "üîπ --- TESTE FINALIZADO ---\n",
      "\n",
      "========================================\n",
      "üìä RELAT√ìRIO DE MEM√ìRIA DO PROCESSO\n",
      "========================================\n",
      "üìâ Inicial:      517.51 MB\n",
      "üìà Final:        1.48 GB\n",
      "üöÄ PICO (Peak):  1.49 GB\n",
      "üíß Diferen√ßa:    1001.74 MB\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from app.tools.context_store import AnalysisContext \n",
    "from app.tools.metrics_agent_tools import get_dataset_health_check, query_anomalous_ids, run_ml_inference_pipeline\n",
    "from app.tools.data_tools import *\n",
    "\n",
    "orchestrator_tools = [\n",
    "      query_mongo_requests, \n",
    "      query_sql_campaigns, \n",
    "      list_avaiable_datasets, \n",
    "      inspect_file_schema, \n",
    "      load_dataset_into_context, \n",
    "      check_context_status, \n",
    "      inspect_file_schema\n",
    "]\n",
    "\n",
    "\n",
    "async def test_full_workflow():\n",
    "    print(\"\\nüîπ --- INICIANDO TESTE DE INTEGRA√á√ÉO (SEM LLM) ---\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 1. SIMULA√á√ÉO DO ORQUESTRADOR (Discovery & Ingestion)\n",
    "    # ==============================================================================\n",
    "    print(\"\\nTesting: 1. Orchestrator - Discovery (SQL)\")\n",
    "    \n",
    "    # Simula o LLM chamando a tool com argumentos\n",
    "    # Nota: Se suas tools usam @tool, use .invoke() ou chame a fun√ß√£o decorada diretamente dependendo da vers√£o do LangChain\n",
    "    try:\n",
    "        # Tenta listar campanhas do Google\n",
    "        campaigns_str = await query_sql_campaigns.ainvoke({\"traffic_source\": \"google\", \"limit\": 2})\n",
    "        print(f\"‚úÖ SQL Result: {campaigns_str}\")\n",
    "        \n",
    "        # HACK PARA O TESTE:\n",
    "        # Como n√£o temos o LLM para ler a string e escolher o hash, vamos pegar um hash 'fake' \n",
    "        # ou extrair da string se o banco estiver conectado.\n",
    "        # Para este teste, vou assumir que voc√™ pegou um hash v√°lido do log acima.\n",
    "        target_hash = \"uw0qfu4a1r\" \n",
    "        print(f\"Usando Hash Alvo: {target_hash}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SQL Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 2. Orchestrator - Ingestion (Mongo -> Context)\")\n",
    "    try:\n",
    "        # Simula o carregamento\n",
    "\n",
    "        status_msg = await query_mongo_requests.ainvoke({\n",
    "            \"hash\": target_hash,\n",
    "            # \"hashes\": campaigns_str, \n",
    "            \"traffic_source\": \"google\"\n",
    "        })\n",
    "        print(f\"‚úÖ Load Result: {status_msg}\")\n",
    "        \n",
    "        # VERIFICA√á√ÉO DE ESTADO (Crucial!)\n",
    "        # Vamos espiar dentro do Singleton para ver se funcionou\n",
    "        try:\n",
    "            print(\"Status: \", AnalysisContext.get_status())\n",
    "            # print(f\"üîé VERIFICA√á√ÉO: Contexto cont√©m {len(df)} linhas. Colunas: {list(df.columns[:3])}...\")\n",
    "        except ValueError:\n",
    "            print(\"‚ùå VERIFICA√á√ÉO FALHOU: Contexto est√° vazio!\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mongo Load Failed: {e}\")\n",
    "        # SE VOC√ä N√ÉO TEM BANCO RODANDO AGORA, DESCOMENTE A LINHA ABAIXO PARA MOCKAR DADOS:\n",
    "        # mock_data_loading() \n",
    "        return\n",
    "\n",
    "\n",
    "    print(\"\\nTesting: 3. Sub-agent - ML Execution\")\n",
    "    try:\n",
    "        # O agente chama sem argumentos, pois pega do Contexto\n",
    "        inference_summary = run_ml_inference_pipeline.invoke({}) \n",
    "        print(f\"‚úÖ Inference Result: {inference_summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ML Pipeline Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 4. Sub-agent - Health Check\")\n",
    "    try:\n",
    "        health_stats = get_dataset_health_check.invoke({})\n",
    "        print(f\"‚úÖ Health Stats: {health_stats}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Health Check Failed: {e}\")\n",
    "\n",
    "    print(\"\\nTesting: 5. Sub-agent - Query Anomalies\")\n",
    "    try:\n",
    "        # Testa buscar IDs com baixa confian√ßa\n",
    "        anomalies = query_anomalous_ids.invoke({\"criteria\": \"low_trust\", \"threshold\": 0.5})\n",
    "        print(f\"‚úÖ Found {len(anomalies)} anomalies. Sample IDs: {anomalies[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query Failed: {e}\")\n",
    "\n",
    "    print(\"\\nüîπ --- TESTE FINALIZADO ---\")\n",
    "\n",
    "# --- MOCK OPCIONAL (Se voc√™ n√£o tiver o Mongo rodando localmente) ---\n",
    "def mock_data_loading():\n",
    "    print(\"‚ö†Ô∏è MOCKING DATA LOADING...\")\n",
    "    data = {\n",
    "        \"id\": range(100),\n",
    "        \"user_agent\": [\"Mozilla/5.0\"] * 50 + [\"Googlebot\"] * 50,\n",
    "        \"url\": [\"/home\"] * 100,\n",
    "        \"label\": [1]*50 + [0]*50 # 1=Human, 0=Bot\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    AnalysisContext.set_mongo_data(df, \"google\")\n",
    "    print(\"‚úÖ Mock data loaded into Context.\")\n",
    "\n",
    "# --- Seu Wrapper de Teste ---\n",
    "async def run_with_monitoring():\n",
    "    monitor = MemoryMonitor(interval=0.1) # Checa a cada 100ms\n",
    "    \n",
    "    monitor.start()\n",
    "    try:\n",
    "        print(\"‚è≥ Executando Workflow...\")\n",
    "        # Chama sua fun√ß√£o original aqui\n",
    "        await test_full_workflow() \n",
    "    finally:\n",
    "        monitor.stop()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n RELAT√ìRIO GPU (VRAM)\")\n",
    "            print(f\"Alocada: {format_bytes(torch.cuda.memory_allocated())}\")\n",
    "            print(f\"Reservada: {format_bytes(torch.cuda.memory_reserved())}\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await run_with_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcae92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.config.container import request_service\n",
    "from app.config.container import campaign_service\n",
    "\n",
    "\n",
    "campaigns = await campaign_service.fetch_recent_active_campaigns(traffic_source=\"google\", limit=50)\n",
    "results = await request_service.fetch_training_sample_by_hashes(campaigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88d1391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "bots      10000\n",
       "unsafe    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results[\"decision\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8faa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-twr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
