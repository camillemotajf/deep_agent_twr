{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45b171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "# Importe suas fun√ß√µes de teste\n",
    "# from test_pipeline import test_full_workflow \n",
    "\n",
    "# --- Utilit√°rio de Formata√ß√£o ---\n",
    "def format_bytes(size):\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return f\"{size:.2f} {power_labels[n]}\"\n",
    "\n",
    "# --- Classe Monitor de Mem√≥ria ---\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.running = False\n",
    "        self.max_rss = 0\n",
    "        self.start_rss = 0\n",
    "        self.end_rss = 0\n",
    "        self._thread = None\n",
    "\n",
    "    def _monitor(self):\n",
    "        while self.running:\n",
    "            # RSS: Resident Set Size (Mem√≥ria RAM f√≠sica usada)\n",
    "            current_rss = self.process.memory_info().rss\n",
    "            if current_rss > self.max_rss:\n",
    "                self.max_rss = current_rss\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        self.start_rss = self.process.memory_info().rss\n",
    "        self.max_rss = self.start_rss\n",
    "        self.running = True\n",
    "        self._thread = threading.Thread(target=self._monitor, daemon=True)\n",
    "        self._thread.start()\n",
    "        print(f\"üìâ Mem√≥ria Inicial: {format_bytes(self.start_rss)}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self._thread:\n",
    "            self._thread.join()\n",
    "        self.end_rss = self.process.memory_info().rss\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"üìä RELAT√ìRIO DE MEM√ìRIA DO PROCESSO\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"üìâ Inicial:      {format_bytes(self.start_rss)}\")\n",
    "        print(f\"üìà Final:        {format_bytes(self.end_rss)}\")\n",
    "        print(f\"üöÄ PICO (Peak):  {format_bytes(self.max_rss)}\")\n",
    "        print(f\"üíß Diferen√ßa:    {format_bytes(self.end_rss - self.start_rss)}\")\n",
    "        print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c54139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from app.tools.context_store import AnalysisContext \n",
    "from app.tools.metrics_agent_tools import get_dataset_health_check, query_anomalous_ids, run_ml_inference_pipeline, choose_emb_conf\n",
    "from app.tools.data_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87ef27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Mem√≥ria Inicial: 3.20 GB\n",
      "‚è≥ Executando Workflow...\n",
      "\n",
      "üîπ --- INICIANDO TESTE DE INTEGRA√á√ÉO (SEM LLM) ---\n",
      "\n",
      "Testing: 1. Orchestrator - Discovery (SQL)\n",
      "‚úÖ SQL Result: success=True campaigns=['yszqvqzj0c', '6b5w3e0qsv'] message=None\n",
      "Usando Hash Alvo: uw0qfu4a1r\n",
      "\n",
      "Testing: 2. Orchestrator - Ingestion (Mongo -> Context)\n",
      "DEBUG [Context]: Mongo Data Loaded. Rows: 688\n",
      "‚úÖ Load Result: success=True message=\"SUCCESS: Loaded 688 requests into AnalysisContext.\\nSources: google | Hashes: 1\\nAction Required: Delegate to 'Metrics Analyst' agent to run ML inference now.\" num_requests=688\n",
      "Status:  Mongo Raw: 688 | ML Processed: Pending\n",
      "\n",
      "Testing: 3.1 Sub-agent - ML Execution\n",
      "G:/Meu Drive/TWR/data/google\n",
      "[DEBUG] Model Path FASTTEXT: G:/Meu Drive/TWR/data/google/fasttext_google.model\n",
      "<app.services.embedding_service.FastTextEmbedder object at 0x00000201D5C9F110>\n",
      "Enter to Fasttext encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Criando Vocabul√°rio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:00<00:00, 4069.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 out of 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vetorizando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:06<00:00, 100.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing encoding\n",
      "Metric val for fasttext: 0.3053992986679077\n",
      "Score para fasttext: 0.3053992986679077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 662.66it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<app.services.embedding_service.TransformerEmbedder object at 0x00000201C58A5B50>\n",
      "Enter to Transformers encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:00<00:00, 3316.79it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:21<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing encoding\n",
      "Metric val for transformers: 0.14213261008262634\n",
      "Score para transformers: 0.14213261008262634\n",
      "Melhor modelo: fasttext | Caminho/Config: G:/Meu Drive/TWR/data/google\n",
      "[DEBUG] Model Path FASTTEXT: G:/Meu Drive/TWR/data/google/fasttext_google.model\n",
      "‚úÖ Inference Result: {'emb_conf': 'fasttext', 'model_path': 'G:/Meu Drive/TWR/data/google'}\n",
      "\n",
      "Testing: 3.2 Sub-agent - ML Execution\n",
      "google\n",
      "Embedding type:  fasttext\n",
      "Enter to Fasttext encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Criando Vocabul√°rio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:00<00:00, 3407.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 out of 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vetorizando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:05<00:00, 117.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing encoding\n",
      "‚è≥ [CACHE MISS] Instanciando e carregando MentorNetPredictor do disco: G:/Meu Drive/TWR/data/google/fasttext/mentor_net_bundle.pth...\n",
      "DEBUG [Context]: ML Results Stored. Rows: 688\n",
      "‚úÖ Inference Result: Inference completed using 'google' model with results: \n",
      "Models Accuracy: 0.9273255813953488Total Error in prediction (possible anomalies): 50Analyzed 688 samples.\n",
      "You can now now:\n",
      "1. Call 'get_dataset_health_check' to see overall performance stats.\n",
      "2. Call 'query_anomalous_ids' to extract specific samples for the Detective Agent.\n",
      "\n",
      "Testing: 4. Sub-agent - Health Check\n",
      "‚úÖ Health Stats: {'total_samples': 688, 'false_positives': 44, 'false_negatives': 6, 'avg_trust': 0.8922286629676819}\n",
      "\n",
      "Testing: 5. Sub-agent - Query Anomalies\n",
      "Found 73 samples matching criteria 'low_trust' with threshold 0.5.\n",
      "       weight      loss  target  pred  ncs\n",
      "8    0.427834  0.077653     1.0   1.0  0.5\n",
      "68   0.001892  0.987114     0.0   1.0  0.2\n",
      "70   0.012552  0.724372     0.0   1.0  0.3\n",
      "91   0.358438  0.992844     0.0   1.0  0.5\n",
      "111  0.011402  0.994066     0.0   1.0  0.3\n",
      "‚úÖ Found 50 anomalies. Sample IDs: [ObjectId('69964815607977cd582244b5'), ObjectId('6992027667e199dbfbc8b8d0'), ObjectId('6991f29067e199dbfbbd0309'), ObjectId('699088c6bd7271637bd677dd'), ObjectId('698fb3debd7271637b45cc12')]\n",
      "\n",
      "üîπ --- TESTE FINALIZADO ---\n",
      "\n",
      "========================================\n",
      "üìä RELAT√ìRIO DE MEM√ìRIA DO PROCESSO\n",
      "========================================\n",
      "üìâ Inicial:      3.20 GB\n",
      "üìà Final:        3.12 GB\n",
      "üöÄ PICO (Peak):  3.20 GB\n",
      "üíß Diferen√ßa:    -82976768.00 \n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orchestrator_tools = [\n",
    "      query_mongo_requests, \n",
    "      query_sql_campaigns, \n",
    "      list_avaiable_datasets, \n",
    "      inspect_file_schema, \n",
    "      load_dataset_into_context, \n",
    "      check_context_status, \n",
    "      inspect_file_schema\n",
    "]\n",
    "\n",
    "async def test_full_workflow():\n",
    "    print(\"\\nüîπ --- INICIANDO TESTE DE INTEGRA√á√ÉO (SEM LLM) ---\")\n",
    "\n",
    "    print(\"\\nTesting: 1. Orchestrator - Discovery (SQL)\")\n",
    "\n",
    "    try:\n",
    "        # Tenta listar campanhas do Google\n",
    "        campaigns_str = await query_sql_campaigns.ainvoke({\"traffic_source\": \"google\", \"limit\": 2})\n",
    "        print(f\"‚úÖ SQL Result: {campaigns_str}\")\n",
    "        \n",
    "        # HACK PARA O TESTE:\n",
    "        # Como n√£o temos o LLM para ler a string e escolher o hash, vamos pegar um hash 'fake' \n",
    "        # ou extrair da string se o banco estiver conectado.\n",
    "        # Para este teste, vou assumir que voc√™ pegou um hash v√°lido do log acima.\n",
    "        target_hash = \"uw0qfu4a1r\" \n",
    "        print(f\"Usando Hash Alvo: {target_hash}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SQL Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 2. Orchestrator - Ingestion (Mongo -> Context)\")\n",
    "    try:\n",
    "        # Simula o carregamento\n",
    "\n",
    "        status_msg = await query_mongo_requests.ainvoke({\n",
    "            \"hash\": target_hash,\n",
    "            # \"hashes\": campaigns_str, \n",
    "            \"traffic_source\": \"google\"\n",
    "        })\n",
    "        print(f\"‚úÖ Load Result: {status_msg}\")\n",
    "        \n",
    "        # VERIFICA√á√ÉO DE ESTADO (Crucial!)\n",
    "        # Vamos espiar dentro do Singleton para ver se funcionou\n",
    "        try:\n",
    "            print(\"Status: \", AnalysisContext.get_status())\n",
    "            # print(f\"üîé VERIFICA√á√ÉO: Contexto cont√©m {len(df)} linhas. Colunas: {list(df.columns[:3])}...\")\n",
    "        except ValueError:\n",
    "            print(\"‚ùå VERIFICA√á√ÉO FALHOU: Contexto est√° vazio!\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mongo Load Failed: {e}\")\n",
    "        # SE VOC√ä N√ÉO TEM BANCO RODANDO AGORA, DESCOMENTE A LINHA ABAIXO PARA MOCKAR DADOS:\n",
    "        # mock_data_loading() \n",
    "        return\n",
    "\n",
    "\n",
    "    print(\"\\nTesting: 3.1 Sub-agent - ML Execution\")\n",
    "    try:\n",
    "        # O agente chama sem argumentos, pois pega do Contexto\n",
    "        choosed_emb = choose_emb_conf.invoke({\"traffic_source\": \"google\", \"df\": AnalysisContext.get_data_from_mongo()}) \n",
    "        print(f\"‚úÖ Inference Result: {choosed_emb}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Chosse embedding pipeline failed {e}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nTesting: 3.2 Sub-agent - ML Execution\")\n",
    "    try:\n",
    "        # O agente chama sem argumentos, pois pega do Contexto\n",
    "        inference_summary = run_ml_inference_pipeline.invoke({\"emb_conf\": choosed_emb}) \n",
    "        print(f\"‚úÖ Inference Result: {inference_summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ML Pipeline Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 4. Sub-agent - Health Check\")\n",
    "    try:\n",
    "        health_stats = get_dataset_health_check.invoke({})\n",
    "        print(f\"‚úÖ Health Stats: {health_stats}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Health Check Failed: {e}\")\n",
    "\n",
    "    print(\"\\nTesting: 5. Sub-agent - Query Anomalies\")\n",
    "    try:\n",
    "        # Testa buscar IDs com baixa confian√ßa\n",
    "        anomalies = query_anomalous_ids.invoke({\"criteria\": \"low_trust\", \"threshold\": 0.5})\n",
    "        print(f\"‚úÖ Found {len(anomalies)} anomalies. Sample IDs: {anomalies[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query Failed: {e}\")\n",
    "\n",
    "    print(\"\\nüîπ --- TESTE FINALIZADO ---\")\n",
    "\n",
    "# --- MOCK OPCIONAL (Se voc√™ n√£o tiver o Mongo rodando localmente) ---\n",
    "def mock_data_loading():\n",
    "    print(\"‚ö†Ô∏è MOCKING DATA LOADING...\")\n",
    "    data = {\n",
    "        \"id\": range(100),\n",
    "        \"user_agent\": [\"Mozilla/5.0\"] * 50 + [\"Googlebot\"] * 50,\n",
    "        \"url\": [\"/home\"] * 100,\n",
    "        \"label\": [1]*50 + [0]*50 # 1=Human, 0=Bot\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    AnalysisContext.set_mongo_data(df, \"google\")\n",
    "    print(\"‚úÖ Mock data loaded into Context.\")\n",
    "\n",
    "# --- Seu Wrapper de Teste ---\n",
    "async def run_with_monitoring():\n",
    "    monitor = MemoryMonitor(interval=0.1) \n",
    "    monitor.start()\n",
    "    try:\n",
    "        print(\"‚è≥ Executando Workflow...\")\n",
    "        await test_full_workflow() \n",
    "    finally:\n",
    "        monitor.stop()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n RELAT√ìRIO GPU (VRAM)\")\n",
    "            print(f\"Alocada: {format_bytes(torch.cuda.memory_allocated())}\")\n",
    "            print(f\"Reservada: {format_bytes(torch.cuda.memory_reserved())}\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await run_with_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import TypedDict, Annotated, List, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    traffic_source: str\n",
    "    target_hash: str\n",
    "    embedding_choice: Any  # Pode ser o dict ou a string que sua tool retorna\n",
    "    health_stats: Any\n",
    "    anomalies: List[str]\n",
    "    status: str\n",
    "\n",
    "# 2. DEFINI√á√ÉO DOS N√ìS (Cada n√≥ executa uma ou mais Tools)\n",
    "\n",
    "async def node_discovery(state: AgentState):\n",
    "    print(\"\\n[Node: Discovery] Buscando campanhas...\")\n",
    "    traffic_source = state.get(\"traffic_source\", \"google\")\n",
    "    \n",
    "    # Chama a Tool SQL\n",
    "    campaigns_str = await query_sql_campaigns.ainvoke({\"traffic_source\": traffic_source, \"limit\": 2})\n",
    "    print(f\"   Campanhas encontradas: {campaigns_str}\")\n",
    "    \n",
    "    # HACK: Simulando a extra√ß√£o do hash (No futuro, o LLM faria isso)\n",
    "    extracted_hash = \"uw0qfu4a1r\" \n",
    "    \n",
    "    return {\"target_hash\": extracted_hash, \"status\": \"discovery_completed\"}\n",
    "\n",
    "async def node_ingestion(state: AgentState):\n",
    "    print(f\"\\n[Node: Ingestion] üì• Carregando dados do Mongo (Hash: {state['target_hash']})...\")\n",
    "    \n",
    "    # Chama a Tool do Mongo (Que internamente salva no AnalysisContext)\n",
    "    status_msg = await query_mongo_requests.ainvoke({\n",
    "        \"hash\": state[\"target_hash\"],\n",
    "        \"traffic_source\": state[\"traffic_source\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"   Status Mongo: {status_msg}\")\n",
    "    return {\"status\": \"data_loaded_to_context\"}\n",
    "\n",
    "def node_ml_pipeline(state: AgentState):\n",
    "    print(\"\\n[Node: ML Pipeline] ü§ñ Executando infer√™ncia...\")\n",
    "    \n",
    "    # 1. Chama a Tool para escolher o embedding (sem passar o DF, a tool pega do Contexto)\n",
    "    choosed_emb = choose_emb_conf.invoke({\"traffic_source\": state[\"traffic_source\"]})\n",
    "    print(f\"   Embedding selecionado: {choosed_emb}\")\n",
    "    \n",
    "    # 2. Roda o pipeline de infer√™ncia com o embedding escolhido\n",
    "    inference_summary = run_ml_inference_pipeline.invoke({\"emb_conf\": choosed_emb})\n",
    "    print(f\"   Resumo ML: {inference_summary}\")\n",
    "    \n",
    "    return {\"embedding_choice\": choosed_emb, \"status\": \"ml_execution_completed\"}\n",
    "\n",
    "def node_health_check(state: AgentState):\n",
    "    print(\"\\n[Node: Health Check] ü©∫ Avaliando integridade dos dados...\")\n",
    "    \n",
    "    # Chama a tool de Health Check\n",
    "    health_stats = get_dataset_health_check.invoke({})\n",
    "    print(f\"   Sa√∫de do Dataset: {health_stats}\")\n",
    "    \n",
    "    return {\"health_stats\": health_stats, \"status\": \"health_check_completed\"}\n",
    "\n",
    "def node_anomalies(state: AgentState):\n",
    "    print(\"\\n[Node: Anomalies] üö® Buscando IDs an√¥malos...\")\n",
    "    \n",
    "    # Chama a tool de anomalias\n",
    "    anomalies = query_anomalous_ids.invoke({\"criteria\": \"low_trust\", \"threshold\": 0.5})\n",
    "    print(f\"   Anomalias encontradas: {len(anomalies)}\")\n",
    "    \n",
    "    return {\"anomalies\": anomalies, \"status\": \"workflow_finished\"}\n",
    "\n",
    "\n",
    "# 3. CONSTRUINDO O GRAFO (Conectando os n√≥s)\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Adicionando os n√≥s ao grafo\n",
    "workflow.add_node(\"discovery\", node_discovery)\n",
    "workflow.add_node(\"ingestion\", node_ingestion)\n",
    "workflow.add_node(\"ml_pipeline\", node_ml_pipeline)\n",
    "workflow.add_node(\"health_check\", node_health_check)\n",
    "workflow.add_node(\"anomalies\", node_anomalies)\n",
    "\n",
    "# Definindo a ordem de execu√ß√£o (Arestas/Edges)\n",
    "workflow.set_entry_point(\"discovery\")\n",
    "workflow.add_edge(\"discovery\", \"ingestion\")\n",
    "workflow.add_edge(\"ingestion\", \"ml_pipeline\")\n",
    "workflow.add_edge(\"ml_pipeline\", \"health_check\")\n",
    "workflow.add_edge(\"health_check\", \"anomalies\")\n",
    "workflow.add_edge(\"anomalies\", END)\n",
    "\n",
    "# Compilando o aplicativo LangGraph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08ef13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Executando Grafo...\n",
      "\n",
      "[Node: Discovery] Buscando campanhas...\n",
      "   Campanhas encontradas: success=True campaigns=['yszqvqzj0c', '6b5w3e0qsv'] message=None\n",
      "‚úÖ Passou pelo n√≥: discovery\n",
      "Estado atualizado: {'target_hash': 'uw0qfu4a1r', 'status': 'discovery_completed'}\n",
      "\n",
      "\n",
      "[Node: Ingestion] üì• Carregando dados do Mongo (Hash: uw0qfu4a1r)...\n",
      "DEBUG [Context]: Mongo Data Loaded. Rows: 688\n",
      "   Status Mongo: success=True message=\"SUCCESS: Loaded 688 requests into AnalysisContext.\\nSources: google | Hashes: 1\\nAction Required: Delegate to 'Metrics Analyst' agent to run ML inference now.\" num_requests=688\n",
      "‚úÖ Passou pelo n√≥: ingestion\n",
      "Estado atualizado: {'status': 'data_loaded_to_context'}\n",
      "\n",
      "\n",
      "[Node: ML Pipeline] ü§ñ Executando infer√™ncia...\n",
      "   Embedding selecionado: {'emb_conf': 'fasttext', 'model_path': 'G:/Meu Drive/TWR/data/google'}\n",
      "google\n",
      "Embedding type:  fasttext\n",
      "Enter to Fasttext encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Criando Vocabul√°rio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:00<00:00, 4091.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11 out of 12 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vetorizando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 688/688 [00:06<00:00, 102.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing encoding\n",
      "DEBUG [Context]: ML Results Stored. Rows: 688\n",
      "   Resumo ML: Inference completed using 'google' model with results: \n",
      "Models Accuracy: 0.9273255813953488Total Error in prediction (possible anomalies): 50Analyzed 688 samples.\n",
      "You can now now:\n",
      "1. Call 'get_dataset_health_check' to see overall performance stats.\n",
      "2. Call 'query_anomalous_ids' to extract specific samples for the Detective Agent.\n",
      "‚úÖ Passou pelo n√≥: ml_pipeline\n",
      "Estado atualizado: {'embedding_choice': {'emb_conf': 'fasttext', 'model_path': 'G:/Meu Drive/TWR/data/google'}, 'status': 'ml_execution_completed'}\n",
      "\n",
      "\n",
      "[Node: Health Check] ü©∫ Avaliando integridade dos dados...\n",
      "   Sa√∫de do Dataset: {'total_samples': 688, 'false_positives': 44, 'false_negatives': 6, 'avg_trust': 0.8922286629676819}\n",
      "‚úÖ Passou pelo n√≥: health_check\n",
      "Estado atualizado: {'health_stats': {'total_samples': 688, 'false_positives': 44, 'false_negatives': 6, 'avg_trust': 0.8922286629676819}, 'status': 'health_check_completed'}\n",
      "\n",
      "\n",
      "[Node: Anomalies] üö® Buscando IDs an√¥malos...\n",
      "Found 73 samples matching criteria 'low_trust' with threshold 0.5.\n",
      "       weight      loss  target  pred  ncs\n",
      "8    0.427834  0.077653     1.0   1.0  0.5\n",
      "68   0.001892  0.987114     0.0   1.0  0.2\n",
      "70   0.012552  0.724372     0.0   1.0  0.3\n",
      "91   0.358438  0.992844     0.0   1.0  0.5\n",
      "111  0.011402  0.994066     0.0   1.0  0.3\n",
      "   Anomalias encontradas: 50\n",
      "‚úÖ Passou pelo n√≥: anomalies\n",
      "Estado atualizado: {'anomalies': [ObjectId('69964815607977cd582244b5'), ObjectId('6992027667e199dbfbc8b8d0'), ObjectId('6991f29067e199dbfbbd0309'), ObjectId('699088c6bd7271637bd677dd'), ObjectId('698fb3debd7271637b45cc12'), ObjectId('698fb3d9bd7271637b45c7f9'), ObjectId('698f68f3bd7271637bfd974f'), ObjectId('698f4f08bd7271637beb9f8e'), ObjectId('698e8967bd7271637b7e72ce'), ObjectId('698e84d2bd7271637b796828'), ObjectId('698e7c4fbd7271637b7149d7'), ObjectId('698e7bafbd7271637b709eff'), ObjectId('698e787bbd7271637b6cfe7d'), ObjectId('698e6f93bd7271637b60a241'), ObjectId('698e319dbd7271637b1df184'), ObjectId('698e1c5ca265b281ec6ac3ee'), ObjectId('698e1c47a265b281ec6abbf7'), ObjectId('698e1af1a265b281ec69b3e3'), ObjectId('698e1acea265b281ec69968e'), ObjectId('698e1aa6a265b281ec697654'), ObjectId('698e1a9ca265b281ec696d76'), ObjectId('698e18eba265b281ec685c44'), ObjectId('698e18d7a265b281ec685252'), ObjectId('698e18d0a265b281ec684e96'), ObjectId('698e1763a265b281ec6754a9'), ObjectId('698e175ca265b281ec675029'), ObjectId('698e165da265b281ec66adb4'), ObjectId('698e15bba265b281ec663f65'), ObjectId('698e1517a265b281ec65c7ea'), ObjectId('698e1440a265b281ec652399'), ObjectId('698e1318a265b281ec647434'), ObjectId('698e12faa265b281ec645fa4'), ObjectId('698e12c1a265b281ec6436ba'), ObjectId('698e12bea265b281ec643402'), ObjectId('698e12b0a265b281ec642938'), ObjectId('698e0e4ca265b281ec605b81'), ObjectId('698e0da6a265b281ec5fe11b'), ObjectId('698e0d53a265b281ec5fa2d9'), ObjectId('698e0d4da265b281ec5f9e27'), ObjectId('698e0d12a265b281ec5f76b1'), ObjectId('698e0d05a265b281ec5f6bcf'), ObjectId('698e0bfca265b281ec5e969a'), ObjectId('698e09fda265b281ec5cdb12'), ObjectId('698e09fba265b281ec5cd99f'), ObjectId('698e09f7a265b281ec5cd767'), ObjectId('698e09e3a265b281ec5cc90b'), ObjectId('698e09e0a265b281ec5cc6b4'), ObjectId('698e09c8a265b281ec5cb3c6'), ObjectId('698e0967a265b281ec5c788b'), ObjectId('698e0862a265b281ec5b9a27')], 'status': 'workflow_finished'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_langgraph_workflow():\n",
    "    inputs = {\"traffic_source\": \"google\"}\n",
    "    \n",
    "    print(\"‚è≥ Executando Grafo...\")\n",
    "    # O stream permite ver passo a passo em tempo real\n",
    "    async for output in app.astream(inputs):\n",
    "        for node_name, state_update in output.items():\n",
    "            print(f\"‚úÖ Passou pelo n√≥: {node_name}\")\n",
    "            print(f\"Estado atualizado: {state_update}\\n\")\n",
    "\n",
    "await test_langgraph_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-twr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
