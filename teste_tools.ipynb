{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45b171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "# Importe suas funÃ§Ãµes de teste\n",
    "# from test_pipeline import test_full_workflow \n",
    "\n",
    "# --- UtilitÃ¡rio de FormataÃ§Ã£o ---\n",
    "def format_bytes(size):\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return f\"{size:.2f} {power_labels[n]}\"\n",
    "\n",
    "# --- Classe Monitor de MemÃ³ria ---\n",
    "class MemoryMonitor:\n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.running = False\n",
    "        self.max_rss = 0\n",
    "        self.start_rss = 0\n",
    "        self.end_rss = 0\n",
    "        self._thread = None\n",
    "\n",
    "    def _monitor(self):\n",
    "        while self.running:\n",
    "            # RSS: Resident Set Size (MemÃ³ria RAM fÃ­sica usada)\n",
    "            current_rss = self.process.memory_info().rss\n",
    "            if current_rss > self.max_rss:\n",
    "                self.max_rss = current_rss\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        self.start_rss = self.process.memory_info().rss\n",
    "        self.max_rss = self.start_rss\n",
    "        self.running = True\n",
    "        self._thread = threading.Thread(target=self._monitor, daemon=True)\n",
    "        self._thread.start()\n",
    "        print(f\"ðŸ“‰ MemÃ³ria Inicial: {format_bytes(self.start_rss)}\")\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self._thread:\n",
    "            self._thread.join()\n",
    "        self.end_rss = self.process.memory_info().rss\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"ðŸ“Š RELATÃ“RIO DE MEMÃ“RIA DO PROCESSO\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"ðŸ“‰ Inicial:      {format_bytes(self.start_rss)}\")\n",
    "        print(f\"ðŸ“ˆ Final:        {format_bytes(self.end_rss)}\")\n",
    "        print(f\"ðŸš€ PICO (Peak):  {format_bytes(self.max_rss)}\")\n",
    "        print(f\"ðŸ’§ DiferenÃ§a:    {format_bytes(self.end_rss - self.start_rss)}\")\n",
    "        print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87ef27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\.venv\\Lib\\site-packages\\motor\\core.py:171: UserWarning: You appear to be connected to a DocumentDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/documentdb\n",
      "  delegate = self.__delegate_class__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garantindo Ã­ndices...\n",
      "ðŸ“‰ MemÃ³ria Inicial: 512.77 MB\n",
      "â³ Executando Workflow...\n",
      "\n",
      "ðŸ”¹ --- INICIANDO TESTE DE INTEGRAÃ‡ÃƒO (SEM LLM) ---\n",
      "\n",
      "Testing: 1. Orchestrator - Discovery (SQL)\n",
      "âœ… SQL Result: ['wag1u37igk', 'imrawov20j']\n",
      "ðŸ‘‰ Usando Hash Alvo: uw0qfu4a1r\n",
      "\n",
      "Testing: 2. Orchestrator - Ingestion (Mongo -> Context)\n",
      "DEBUG [Context]: Mongo Data Loaded. Rows: 1000\n",
      "âœ… Load Result: SUCCESS: Loaded 1000 requests into AnalysisContext.\n",
      "Sources: google | Hashes: 1\n",
      "Action Required: Delegate to 'Metrics Analyst' agent to run ML inference now.\n",
      "Status:  Mongo Raw: 1000 | ML Processed: Pending\n",
      "\n",
      "Testing: 3. Sub-agent - ML Execution\n",
      "                          _id                datetime decision  \\\n",
      "0    698e1efda265b281ec6cb12b 2026-02-12 18:42:05.331   unsafe   \n",
      "1    698e1e8da265b281ec6c52fc 2026-02-12 18:40:13.101   unsafe   \n",
      "2    698e1cffa265b281ec6b1ce8 2026-02-12 18:33:35.886     bots   \n",
      "3    698e1cfca265b281ec6b1a25 2026-02-12 18:33:32.065     bots   \n",
      "4    698e1c5ca265b281ec6ac3ee 2026-02-12 18:30:52.897   unsafe   \n",
      "..                        ...                     ...      ...   \n",
      "995  698dcd91a265b281ec2dab02 2026-02-12 12:54:41.236     bots   \n",
      "996  698dcd8ea265b281ec2da9d8 2026-02-12 12:54:38.784   unsafe   \n",
      "997  698dcd53a265b281ec2d8939 2026-02-12 12:53:39.213   unsafe   \n",
      "998  698dcd4aa265b281ec2d8515 2026-02-12 12:53:30.079   unsafe   \n",
      "999  698dcd3ea265b281ec2d8055 2026-02-12 12:53:18.335   unsafe   \n",
      "\n",
      "                                               headers  \\\n",
      "0    {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "1    {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "2    {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "3    {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "4    {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "..                                                 ...   \n",
      "995  {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "996  {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "997  {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "998  {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "999  {\"Host\":\"www.rebeccabonbonloja.com\",\"X-Request...   \n",
      "\n",
      "                                               request  \n",
      "0    {\"cr\":\"796830389473\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "1    {\"cr\":\"796830380800\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "2    {\"cr\":\"796911331385\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "3    {\"cr\":\"796512523653\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "4    {\"cr\":\"796830380800\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "..                                                 ...  \n",
      "995  {\"cr\":\"796830389473\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "996  {\"cr\":\"796830389473\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "997  {\"cr\":\"796911330419\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "998  {\"cr\":\"796830389473\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "999  {\"cr\":\"796999335039\",\"plc\":\"\",\"mtx\":\"b\",\"rdn\":...  \n",
      "\n",
      "[1000 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 300.07it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:32<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG [Context]: ML Results Stored. Rows: 1000\n",
      "checando se a tool de inferencia salva dos dados: 1000\n",
      "âœ… Inference Result: Inference completed using 'google' model with results: \n",
      "Models Accuracy: 0.277Total Error in prediction (possible anomalies): 723Analyzed 1000 samples.\n",
      "You can now now:\n",
      "1. Call 'get_dataset_health_check' to see overall performance stats.\n",
      "2. Call 'query_anomalous_ids' to extract specific samples for the Detective Agent.\n",
      "\n",
      "Testing: 4. Sub-agent - Health Check\n",
      "âœ… Health Stats: {'total_samples': 1000, 'false_positives': 1, 'false_negatives': 722, 'avg_trust': 0.3169092833995819}\n",
      "\n",
      "Testing: 5. Sub-agent - Query Anomalies\n",
      "âŒ Query Failed: 'id'\n",
      "\n",
      "ðŸ”¹ --- TESTE FINALIZADO ---\n",
      "\n",
      "========================================\n",
      "ðŸ“Š RELATÃ“RIO DE MEMÃ“RIA DO PROCESSO\n",
      "========================================\n",
      "ðŸ“‰ Inicial:      512.77 MB\n",
      "ðŸ“ˆ Final:        1.01 GB\n",
      "ðŸš€ PICO (Peak):  1.02 GB\n",
      "ðŸ’§ DiferenÃ§a:    521.10 MB\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from app.tools.context_store import AnalysisContext \n",
    "from deepagents import create_deep_agent\n",
    "from app.tools.metrics_agent_tools import get_dataset_health_check, query_anomalous_ids, run_ml_inference_pipeline\n",
    "from app.tools.data_tools import *\n",
    "\n",
    "orchestrator_tools = [\n",
    "      query_mongo_requests, \n",
    "      query_sql_campaigns, \n",
    "      list_avaiable_datasets, \n",
    "      inspect_file_schema, \n",
    "      load_dataset_into_context, \n",
    "      check_context_status, \n",
    "      inspect_file_schema\n",
    "]\n",
    "\n",
    "\n",
    "async def test_full_workflow():\n",
    "    print(\"\\nðŸ”¹ --- INICIANDO TESTE DE INTEGRAÃ‡ÃƒO (SEM LLM) ---\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 1. SIMULAÃ‡ÃƒO DO ORQUESTRADOR (Discovery & Ingestion)\n",
    "    # ==============================================================================\n",
    "    print(\"\\nTesting: 1. Orchestrator - Discovery (SQL)\")\n",
    "    \n",
    "    # Simula o LLM chamando a tool com argumentos\n",
    "    # Nota: Se suas tools usam @tool, use .invoke() ou chame a funÃ§Ã£o decorada diretamente dependendo da versÃ£o do LangChain\n",
    "    try:\n",
    "        # Tenta listar campanhas do Google\n",
    "        campaigns_str = await query_sql_campaigns.ainvoke({\"traffic_source\": \"google\", \"limit\": 2})\n",
    "        print(f\"âœ… SQL Result: {campaigns_str}\")\n",
    "        \n",
    "        # HACK PARA O TESTE:\n",
    "        # Como nÃ£o temos o LLM para ler a string e escolher o hash, vamos pegar um hash 'fake' \n",
    "        # ou extrair da string se o banco estiver conectado.\n",
    "        # Para este teste, vou assumir que vocÃª pegou um hash vÃ¡lido do log acima.\n",
    "        target_hash = \"uw0qfu4a1r\" \n",
    "        print(f\"ðŸ‘‰ Usando Hash Alvo: {target_hash}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ SQL Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 2. Orchestrator - Ingestion (Mongo -> Context)\")\n",
    "    try:\n",
    "        # Simula o carregamento\n",
    "\n",
    "        status_msg = await query_mongo_requests.ainvoke({\n",
    "            \"hash\": target_hash,\n",
    "            # \"hashes\": campaigns_str, \n",
    "            \"traffic_source\": \"google\"\n",
    "        })\n",
    "        print(f\"âœ… Load Result: {status_msg}\")\n",
    "        \n",
    "        # VERIFICAÃ‡ÃƒO DE ESTADO (Crucial!)\n",
    "        # Vamos espiar dentro do Singleton para ver se funcionou\n",
    "        try:\n",
    "            print(\"Status: \", AnalysisContext.get_status())\n",
    "            # print(f\"ðŸ”Ž VERIFICAÃ‡ÃƒO: Contexto contÃ©m {len(df)} linhas. Colunas: {list(df.columns[:3])}...\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ VERIFICAÃ‡ÃƒO FALHOU: Contexto estÃ¡ vazio!\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Mongo Load Failed: {e}\")\n",
    "        # SE VOCÃŠ NÃƒO TEM BANCO RODANDO AGORA, DESCOMENTE A LINHA ABAIXO PARA MOCKAR DADOS:\n",
    "        # mock_data_loading() \n",
    "        return\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 2. SIMULAÃ‡ÃƒO DO SUB-AGENTE (Analysis & Inference)\n",
    "    # ==============================================================================\n",
    "    print(\"\\nTesting: 3. Sub-agent - ML Execution\")\n",
    "    try:\n",
    "        # O agente chama sem argumentos, pois pega do Contexto\n",
    "        inference_summary = run_ml_inference_pipeline.invoke({}) \n",
    "        print(f\"âœ… Inference Result: {inference_summary}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ML Pipeline Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTesting: 4. Sub-agent - Health Check\")\n",
    "    try:\n",
    "        health_stats = get_dataset_health_check.invoke({})\n",
    "        print(f\"âœ… Health Stats: {health_stats}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Health Check Failed: {e}\")\n",
    "\n",
    "    print(\"\\nTesting: 5. Sub-agent - Query Anomalies\")\n",
    "    try:\n",
    "        # Testa buscar IDs com baixa confianÃ§a\n",
    "        anomalies = query_anomalous_ids.invoke({\"criteria\": \"low_trust\", \"threshold\": 0.5})\n",
    "        print(f\"âœ… Found {len(anomalies)} anomalies. Sample IDs: {anomalies[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query Failed: {e}\")\n",
    "\n",
    "    print(\"\\nðŸ”¹ --- TESTE FINALIZADO ---\")\n",
    "\n",
    "# --- MOCK OPCIONAL (Se vocÃª nÃ£o tiver o Mongo rodando localmente) ---\n",
    "def mock_data_loading():\n",
    "    print(\"âš ï¸ MOCKING DATA LOADING...\")\n",
    "    data = {\n",
    "        \"id\": range(100),\n",
    "        \"user_agent\": [\"Mozilla/5.0\"] * 50 + [\"Googlebot\"] * 50,\n",
    "        \"url\": [\"/home\"] * 100,\n",
    "        \"label\": [1]*50 + [0]*50 # 1=Human, 0=Bot\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    AnalysisContext.set_mongo_data(df, \"google\")\n",
    "    print(\"âœ… Mock data loaded into Context.\")\n",
    "\n",
    "# --- Seu Wrapper de Teste ---\n",
    "async def run_with_monitoring():\n",
    "    monitor = MemoryMonitor(interval=0.1) # Checa a cada 100ms\n",
    "    \n",
    "    monitor.start()\n",
    "    try:\n",
    "        print(\"â³ Executando Workflow...\")\n",
    "        # Chama sua funÃ§Ã£o original aqui\n",
    "        await test_full_workflow() \n",
    "    finally:\n",
    "        monitor.stop()\n",
    "\n",
    "# --- Se vocÃª usar PyTorch (GPU/VRAM) ---\n",
    "def check_gpu_memory():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"\\n RELATÃ“RIO GPU (VRAM)\")\n",
    "            print(f\"Alocada: {format_bytes(torch.cuda.memory_allocated())}\")\n",
    "            print(f\"Reservada: {format_bytes(torch.cuda.memory_reserved())}\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await run_with_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcae92b",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m campaign_service\n\u001b[32m      5\u001b[39m campaigns = \u001b[38;5;28;01mawait\u001b[39;00m campaign_service.fetch_recent_active_campaigns(traffic_source=\u001b[33m\"\u001b[39m\u001b[33mgoogle\u001b[39m\u001b[33m\"\u001b[39m, limit=\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m request_service.fetch_training_sample_by_hashes(campaigns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\app\\services\\request_service.py:37\u001b[39m, in \u001b[36mRequestService.fetch_training_sample_by_hashes\u001b[39m\u001b[34m(self, hashes, limit_each)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hashes:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.repository.get_training_sample_by_hashes(\n\u001b[32m     38\u001b[39m     hashes=hashes, \n\u001b[32m     39\u001b[39m     limit_each=limit_each\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Camille\\Documents\\TWR\\deep_agents_twr\\app\\repositories\\mongo_repository.py:61\u001b[39m, in \u001b[36mMongoRepository.get_training_sample_by_hashes\u001b[39m\u001b[34m(self, hashes, limit_each)\u001b[39m\n\u001b[32m     50\u001b[39m query_bots = {\n\u001b[32m     51\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mmetadata.site\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33m$in\u001b[39m\u001b[33m\"\u001b[39m: hashes},\n\u001b[32m     52\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mdecision\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33m$in\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mbots\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m     53\u001b[39m } \n\u001b[32m     56\u001b[39m query_unsafe = {\n\u001b[32m     57\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mmetadata.site\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33m$in\u001b[39m\u001b[33m\"\u001b[39m: hashes},\n\u001b[32m     58\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mdecision\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33m$in\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m     59\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     62\u001b[39m       \u001b[38;5;28mself\u001b[39m.collection.find(query_bots, projection)\n\u001b[32m     63\u001b[39m       .limit(limit_each)\n\u001b[32m     64\u001b[39m       .sort(\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     65\u001b[39m       .to_list(),\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m       \u001b[38;5;28mself\u001b[39m.collection.find(query_unsafe, projection)\n\u001b[32m     68\u001b[39m       .limit(limit_each)\n\u001b[32m     69\u001b[39m       .sort(\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m       .to_list(),\n\u001b[32m     71\u001b[39m )\n\u001b[32m     73\u001b[39m bots_list = results[\u001b[32m0\u001b[39m]\n\u001b[32m     74\u001b[39m unsafe_list = results[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from app.config.container import request_service\n",
    "from app.config.container import campaign_service\n",
    "\n",
    "\n",
    "campaigns = await campaign_service.fetch_recent_active_campaigns(traffic_source=\"google\", limit=50)\n",
    "results = await request_service.fetch_training_sample_by_hashes(campaigns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d1391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decision\n",
       "bots      10000\n",
       "unsafe    10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results[\"decision\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8faa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-twr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
